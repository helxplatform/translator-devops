apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "name-lookup.fullname" . }}-configmap
  labels:
    {{- include "name-lookup.labels" . | nindent 4 }}
data:
  download.sh: |-
    #!/bin/sh
    set -xa
    DATA_DIR="/var/solr/data"
    DOWNLOAD_DIR="/var/solr/download"
    BACKUP_NAME="snapshot.backup"
    BACKUP_ZIP="${BACKUP_NAME}.tar.gz"
    BACKUP_URL="{{ .Values.dataUrl }}"

    # Delete the download directory.
    rm -rf $DOWNLOAD_DIR
    mkdir -p $DOWNLOAD_DIR
    wget -nv -O $DOWNLOAD_DIR/$BACKUP_ZIP $BACKUP_URL

    # Download
    rm -rf $DATA_DIR
    mkdir -p $DATA_DIR
    cd $DATA_DIR
    tar -xf $DOWNLOAD_DIR/$BACKUP_ZIP -C $DATA_DIR
    rm -rf $DOWNLOAD_DIR
  restore.sh: |-
    #!/bin/sh

    BLOCKLIST_DIR="/var/blocklist"
    BLOCKLIST_CHUNK_SIZE=500

    # Download the Blocklist file. We do this before turning on xtrace (`set -xa`) so that we don't record
    # the GitHub authorization token in the Kubernetes logs.
    {{ if .Values.blocklist.url }}
      wget \
        -O ${BLOCKLIST_DIR}/blocklist.txt \
        {{ if .Values.blocklist.secrets.github_personal_access_token}}--header "Authorization: Token $GITHUB_PERSONAL_ACCESS_TOKEN" \{{ end }}
        {{ .Values.blocklist.url }}

      CURIE_LIST=$(xargs -I{lin} echo \"{lin}\" < ${BLOCKLIST_DIR}/blocklist.txt)
      echo CURIEs on the Blocklist: ${CURIE_LIST}.
      echo Blocklist CURIEs will be chunked into chunks of ${BLOCKLIST_CHUNK_SIZE}.
    {{ end }}

    # Turn on xtrace and set up config variables.
    set -xa
    COLLECTION_NAME="name_lookup"
    SOLR_SERVER=http://{{ include "name-lookup.fullname" . }}-solr-svc:{{ .Values.solr.service.port }}

    # liveliness check

    HEALTH_ENDPOINT=http://{{ include "name-lookup.fullname" . }}-solr-svc:{{ .Values.solr.service.port }}/solr/admin/cores?action=STATUS
    response=$(wget --spider --server-response ${HEALTH_ENDPOINT} 2>&1 | grep "HTTP/" | awk '{ print $2 }') >&2
    until [ "$response" = "200" ]; do
      response=$(wget --spider --server-response ${HEALTH_ENDPOINT} 2>&1 | grep "HTTP/" | awk '{ print $2 }') >&2
      echo "  -- SOLR  is unavailable - sleeping"
      sleep 3
    done

    # solr is ready Now we create collection if it doesn't exist

    EXISTS=$(wget -O - ${SOLR_SERVER}/solr/admin/collections?action=LIST | grep name_lookup)

    # create collection / shard
    if [ -z "$EXISTS" ]
    then
      wget -O- ${SOLR_SERVER}/solr/admin/collections?action=CREATE'&'name=${COLLECTION_NAME}'&'numShards=1'&'replicationFactor=1
      sleep 3
    fi

    # Setup fields for search
    wget --post-data '{"set-user-property": {"update.autoCreateFields": "false"}}' \
        --header='Content-Type:application/json' \
        -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/config
    sleep 1

    # Restore data
    BACKUP_NAME="backup"
    CORE_NAME=${COLLECTION_NAME}_shard1_replica_n1
    RESTORE_URL=${SOLR_SERVER}/solr/${CORE_NAME}/replication?command=restore'&'location=/var/solr/data/var/solr/data/'&'name=${BACKUP_NAME}
    wget -O - $RESTORE_URL
    sleep 10
    RESTORE_STATUS=$(wget -q -O - ${SOLR_SERVER}/solr/${CORE_NAME}/replication?command=restorestatus 2>&1 | grep "success") >&2
    echo "Restore status: " $RESTORE_STATUS
    until [ ! -z $RESTORE_STATUS ] ; do
      echo "restore not done , probably still loading. Note: if this takes too long please check solr health"
      RESTORE_STATUS=$(wget -O - ${SOLR_SERVER}/solr/${CORE_NAME}/replication?command=restorestatus 2>&1 | grep "success") >&2
      sleep 10
    done
    echo "restore done"
    wget --post-data '{
              "add-field-type" : {
                  "name": "LowerTextField",
                  "class": "solr.TextField",
                  "positionIncrementGap": "100",
                  "analyzer": {
                      "tokenizer": {
                          "class": "solr.StandardTokenizerFactory"
                      },
                      "filters": [{
                          "class": "solr.LowerCaseFilterFactory"
                      }]
                  }
              }}' \
        --header='Content-Type:application/json' \
        -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/schema
    sleep 1
    # exactish type taken from https://stackoverflow.com/a/29105025/27310
    wget --post-data '{
              "add-field-type" : {
                  "name": "exactish",
                  "class": "solr.TextField",
                  "analyzer": {
                      "tokenizer": {
                          "class": "solr.KeywordTokenizerFactory"
                      },
                      "filters": [{
                          "class": "solr.LowerCaseFilterFactory"
                      }]
                  }
              }}' \
        --header='Content-Type:application/json' \
        -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/schema
    sleep 1
    wget --post-data '{
                "add-field": [
                    {
                        "name":"names",
                        "type":"LowerTextField",
                        "stored": true,
                        "multiValued": true
                    },
                    {
                        "name":"curie",
                        "type":"string",
                        "stored":true
                    },
                    {
                        "name": "preferred_name",
                        "type": "LowerTextField",
                        "stored": true
                    },
                    {
                        "name": "preferred_name_exactish",
                        "type": "exactish",
                        "indexed": true,
                        "stored": false,
                        "multiValued": false
                    },
                    {
                        "name": "types",
                        "type": "string",
                        "stored": true,
                        "multiValued": true
                    },
                    {
                        "name": "shortest_name_length",
                        "type": "pint",
                        "stored": true
                    },
                    {
                        "name": "curie_suffix",
                        "type": "plong",
                        "docValues": true,
                        "stored": true,
                        "required": false,
                        "sortMissingLast": true
                    },
                    {
                        "name": "taxa",
                        "type": "string",
                        "stored": true,
                        "multiValued": true
                    },
                    {
                        "name": "clique_identifier_count",
                        "type": "pint",
                        "stored": true
                    }
                ]
            }' \
          --header='Content-Type:application/json' \
          -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/schema
    sleep 1
    wget --post-data '{
              "add-copy-field" : {
                "source": "preferred_name",
                "dest": "preferred_name_exactish"
              }}' \
        --header='Content-Type:application/json' \
        -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/schema
    sleep 1

    # Delete the blocklist terms from the Solr database.
    {{ if .Values.blocklist.url }}

    BLOCKLIST_DIR=/var/blocklist

    # Split the blocklist into files of 500 CURIEs each.
    rm -rf ${BLOCKLIST_DIR}/blocklist_*
    split -l ${BLOCKLIST_CHUNK_SIZE} ${BLOCKLIST_DIR}/blocklist.txt ${BLOCKLIST_DIR}/blocklist_

    # Delete each blocklist.
    for file in ${BLOCKLIST_DIR}/blocklist_*; do
      CURIE_LIST=$(xargs -I{lin} echo \\\"{lin}\\\" < ${file})

      SOLR_DELETE="{
        'delete': {
          'query': 'curie:(${CURIE_LIST})'
        }}"
      echo SOLR delete query for ${file}: ${SOLR_DELETE}

      wget --post-data "${SOLR_DELETE}" \
      --header='Content-Type:application/json' \
      -O- ${SOLR_SERVER}/solr/${COLLECTION_NAME}/update?commit=true

      sleep 10
    done
    {{ end }}
 
    exit 0
