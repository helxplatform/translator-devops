# Default values for name-lookup.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.



webServer:
  replicaCount: 1
  service:
    port: 2433
    type: ClusterIP
  # container port
  port: 2433
  image:
    repository: ghcr.io/translatorsri/nameresolution
    tag: v1.3.14

dataUrl: "https://stars.renci.org/var/babel_outputs/2024jul13/nameres/snapshot.backup.tar.gz"
forceRun: false

busybox:
  image:
    repository: busybox
    tag: "1.35"

# busybox:
#   image:
#     repository: busybox
#     tag: latest

solr:
  service:
    port: 8983
    type: ClusterIP
  # container port
  port: 8983
  image:
    repository: solr
    tag: 9.1
  resources:
    # We tried increasing this to 200Gi. It looks like Solr used the following
    # amounts of memory (as per Sterling Grafana):
    #   - Memory Usage (WSS): 13Gi
    #   - Memory Usage (RSS): 11Gi
    #   - Memory Usage (Cache): 111Gi
    #
    # I think the "cache" here refers to the OS-level page cache, which makes
    # sense, as the Solr indexes occupy around 127Gi.
    #
    # Therefore, I suspect that Solr needs about 32-64Gi to load its indexes
    # and other important content, but will use other memory if it is available
    # to cache content. We'll set this to 64Gi for now, but we should try
    # bombarding these instances with queries to check how valuable increasing
    # the memory to Solr helps with performance.
    #
    requests:
      memory: "16Gi"
      cpu: 1000m
    limits:
      memory: "32Gi"
      cpu: 4000m

  # You can control the nodeSelector/affinity/tolerations settings for Solr with the following settings.
  # Other pods (web, restore, backup) are controlled via app.nodeSelector/affinity/tolerations below.
  nodeSelector:
  affinity:
  tolerations:

  heap_mem: "-Xms30G -Xmx30G"
  gc: "-XX:NewSize=4G -XX:MaxNewSize=4G -XX:+UseG1GC -XX:MaxGCPauseMillis=1000 -XX:+UnlockExperimentalVMOptions -XX:G1MaxNewSizePercent=40 -XX:G1NewSizePercent=5 -XX:G1HeapRegionSize=32M -XX:InitiatingHeapOccupancyPercent=90"

  # As of Babel 2023jul13, we need 130G to store the
  # uncompressed backup + 131G Solr database
  # So 300Gi should be enough for the time being.
  storage: 400Gi

  # As far as I know, initresources are only used by the download
  # pod that downloads the Solr backup, which is network-bound,
  # not CPU or memory bound. This used to be 32Gi, but I'm going
  # to reduce it to 1-5Gi. Please increase this if it looks like
  # it'll be helpful.
  initresources:
    requests:
      memory: 1Gi
      cpu: 1000m
    limits:
      memory: 5Gi
      cpu: 2000m

# You can control the nodeSelector/affinity/tolerations settings for all pods (web, restore, backup) except Solr with the following settings.
app:
  serverName: "infores:sri-name-resolver"
  otel:
    enabled: false
    jaegerHost:
    jaegerPort:
  nodeSelector:
  affinity:
  tolerations:
  resources:
    requests:
      memory: "300M"
      cpu: 250m
    limits:
      memory: "512M"
      cpu: 500m

nameOverride: ""
fullnameOverride: ""

ingress:
  annotations:
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
  pathType: "ImplementationSpecific"
  host:
  class:

blocklist:
  # The URL from where to download the blocklist. This should be a simple text file, where each line contains a CURIE.
  # (If this field is blank, all blocklist functionality will be turned off.)
  url:
  # If a GitHub Personal Access Token is necessary to download the above file, it should be set
  # in `blocklist.secrets.github_personal_access_token` in an encrypted file. You can also
  # set this on Kubernetes by running:
  #   kubectl -n $NAMESPACE create secret generic {{name-lookup.fullname}}-secrets --from-literal=github_personal_access_token=github_pat_...
  # The storage necessary to store the blocklist file. This is currently an ephemeral volume, and because we need
  # to use `split` to split this into 500 ID chunks, we actually need TWICE the amount of storage needed to store
  # the blocklist. Luckily, the blocklist is about 7KB right now, so 100Mi should be future proof for a while.
  storage: 100Mi

x_trapi:
  maturity:
    value: "maturity"
  location:
    value: "location"
